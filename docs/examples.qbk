[/=============================================================================
Copyright (C) 2012 Adrian Serio
Copyright (C) 2012 Vinay C Amatya

Distributed under the Boost Software License, Version 1.0. (See accompanying
file LICENSE_1_0.txt or copy at http://www.boost.org/LICENSE_1_0.txt)
=============================================================================/]

[section:examples Examples]

The following sections of our tutorial analyzes some examples to help you get
familiar with the HPX style of programming. We start off with simple examples
that utilize basic HPX elements and then begin to expose the reader to the more
complex, yet powerful, HPX concepts.

[note The instructions for building and running the examples currently 
      only cover Unix variants.]

[/Proofread by:]
[/Adrian Serio 3-13-12]
[/Phillip LeBlanc 3-13-12]

[/////////////////////////////////////////////////////////////////////////////]
[section:fibonacci Fibonacci]

The Fibonacci sequence is a sequence of numbers starting with 0 and 1 where
every subsequent number is the sum of the previous two numbers. In this
example, we will use HPX to calculate the value of the n-th element of the
Fibonacci sequence. In order to compute this problem in parallel, we will use
a facility known as a Future.

As shown in the [link examples.future_schematics figure] below, a Future
encapsulates a delayed computation. It acts as
a proxy for a result initially not known, most of the time because the
computation of the result has not completed yet. The Future synchronizes the
access of this value by optionally suspending any HPX-threads requesting the
result until the value is available. When a Future is created, it spawns a
new HPX-thread (either remotely with a parcel or locally by placing it into
the thread queue) which, when run, will execute the action associated with
the Future. The arguments of the action are bound when the Future is created.

[fig future_schematics.png..Schematic of a Future execution..examples.future_schematics]

Once the action has finished executing, a write operation is performed on the
Future. The write operation marks the Future as completed, and optionally
stores data returned by the action. When the result of the delayed computation
is needed, a read operation is performed on the Future. If the Future's action
hasn't completed when a read operation is performed on it, the reader
HPX-thread is suspended until the Future is ready. The Future facility allows
HPX to schedule work early in a program so that when the function value is
needed it will already be calculated and available. We use this property in our
Fibonacci example below to enable its parallel execution.

[heading Setup]

[teletype]

The source code for this example can be found [hpx_link examples/quickstart/fibonacci.cpp..here]. 

To compile this program, go to your HPX build directory (see __getting_started__
for information on configuring and building HPX) and enter:

``
    $ make examples.quickstart.fibonacci
``

To run the program type:

``
    $ ./bin/fibonacci
``

This should print (time should be approximate):

``
    fibonacci(10) == 55
    elapsed time: 0.00186288 [s]
``

This run used the default settings, which calculate the tenth element of the
Fibonacci sequence. To declare which Fibonacci value you want to calculate, use
the [^--n-value] option. Additionally you can use the
[hpx_cmdline [^--hpx:threads]] option to declare how many OS-threads you
wish to use when running the program. For instance, running:

```
    $ ./bin/fibonacci --n-value 20 ``[link hpx.tutorial.getting_started.commandline --hpx:threads]`` 4
```

Will yield:

``
    fibonacci(20) == 6765
    elapsed time: 0.233827 [s]
``

[c++]

[heading Walkthrough]

[c++]

Now that you have compiled and run the code, let's look at how the code works.
Since this code is written in C++, we will begin with the `main()` function.
Here you can see that in HPX, `main()` is only used to initialize the runtime
system. It is important to note that application-specific command line options
are defined here. HPX uses __boost_program_options__ for command line
processing. You can see that our programs [^--n-value] option is set by calling
the `add_options()` method on an instance of
`boost::program_options::options_description`.  The default value of the
variable is set to 10. This is why when we ran the program for the first time
without using the [^--n-value] option the program returned the 10th value of
the Fibonacci sequence. The constructor argument of the description is the text
that appears when a user uses the [hpx_cmdline [^--help]] option to see what
command line options are available. `HPX_APPLICATION_STRING` is a macro that
expands to a string constant containing the name of the HPX application
currently being compiled.

[import ../examples/quickstart/fibonacci.cpp]
[fib_main]

The `hpx::init()` function in `main()` starts the runtime system, and invokes
`hpx_main()` as the first HPX-thread. Below we can see that the basic program
is simple. The command line option [^--n-value] is read in, a timer
(`hpx::util::high_resolution_timer`) is set up to record the time it takes to
do the computation, an `hpx::lcos::future<>` is created, the `get()` function 
on the `hpx::lcos::future<>` is called to retrieve or wait for the result, and
the answer is printed out.

[fib_hpx_main]

Upon a closer look we see that the `hpx::lcos::future<>` we created is assigned
the return of `hpx::lcos::async<fibonacci_action>(hpx::find_here(), n)`.
`hpx::lcos::async<>()` takes an action, in this case `fibonacci_action`, and
asynchronously kicks of the computation of the action, returning a future which
represents the result of the computation. But wait, what is an action? And what
is this `fibonacci_action`? For starters, an action is a wrapper for a
function. By wrapping functions, HPX can send packets of work to different
processing units. These vehicles allow users to calculate work now, later, or
on certain nodes. The first argument to `hpx::lcos::async<>()` is the location
where the action should be run. In this case, we just want to run the action on
the machine that we are currently on, so we use `hpx::find_here()`. To further
understand this we turn to the code to find where `fibonacci_action` was
defined:

[fib_action]

In this block of code the function `fibonacci()` is declared. After the
declaration, the function is wrapped in a `hpx::actions::plain_result_action1<>`.
A plain action is the most basic form of action. Plain actions wrap simple global
functions which are not associated with any particular object (we will discuss
other types of actions in the __accumulator_example__). The name of the action
is tells us about the arity and nature of the functions return type. `1` signifies that the
action takes 1 argument, and `result` indicates that the action returns a non-`void`
value. 

This picture should now start making sense. The function `fibonacci()` is
wrapped in an action `hpx::actions::plain_result_action1<>`, which was spawned
by `hpx::lcos::async<>()`, which returns a future. Now, lets look at the
function `fibonacci()`:

[fib_func]

On first inspection this block of code is straightforward. First, `if (n < 2)`,
meaning n is 0 or 1, then we return 0 or 1 (recall the first element
of the Fibonacci sequence is 0 and the second is 1). If n is larger than 1, then
we spawn two futures, `n1` and `n2`. Each of these futures represents an asynchronous,
recursive call to `fibonacci()`. After we've created both futures, we wait for both
of them to finish computing, and then we add them together, and return that value
as our result. The recursive call tree will continue until n is equal to 0 or 1,
at which point the value can be returned because it is implicitly known. 
When this termination condition is reached, the futures can then be added up,
producing the n-th value of the Fibonacci sequence.

[endsect] [/Fibonacci]

[/Proofread by:]
[/Adrian Serio 3-13-12]
[/Phillip LeBlanc 3-13-12]

[/////////////////////////////////////////////////////////////////////////////]
[section:hello_world Hello World]

[teletype]

This program will print out a hello world message on every OS-thread on every
locality. The output will look something like this:

``
    hello world from OS-thread 1 on locality 0
    hello world from OS-thread 1 on locality 1
    hello world from OS-thread 0 on locality 0
    hello world from OS-thread 0 on locality 1
``

[heading Setup]

The source code for this example can be found [hpx_link examples/quickstart/hello_world.cpp..here]. 

To compile this program, go to your HPX build directory (see __getting_started__
for information on configuring and building HPX) and enter:

``
    $ make examples.quickstart.hello_world
``

To run the program type:

``
    $ ./bin/hello_world
``

This should print:

``
    hello world from OS-thread 0 on locality 0
``

To use more OS-threads use the command line option [hpx_cmdline [^--hpx:threads]]
and type the number of threads that you wish to use. For example, typing:

```
    $ ./bin/hello_world ``[link hpx.tutorial.getting_started.commandline --hpx:threads]`` 2
```

will yield:

``
    hello world from OS-thread 1 on locality 0
    hello world from OS-thread 0 on locality 0
``

Notice how the ordering of the two print statements will change with
subsequent runs. To run this program on multiple localities please see the
__pbs__ documentation.

[heading Walkthrough]

[c++]

Now that you have compiled and run the code, lets look at how the code works,
beginning with `main()` and `hpx_main()`:

[import ../examples/quickstart/hello_world.cpp]
[hello_world_main]
[hello_world_hpx_main]

In this excerpt of the code we again see the use of futures. This time
the futures are stored in a vector so that they can easily be accessed.
`hpx::lcos::wait()` is a family of functions that wait on for an `std::vector<>`
of futures to become ready. In this piece of code, we are using the synchronous
version of `hpx::lcos::wait()`, which takes one argument (the `std::vector<>` of
futures to wait on). This function will not return until all the futures in the
vector have been executed.

In the __fibonacci_example__, we used `hpx::find_here()` to specified the target'
of our actions. Here, we instead use `hpx::find_all_localities()`, which returns
an `std::vector<>` containing the identifiers of all the machines in the system,
including the one that we are on. 

As in the __fibonacci_example__ our futures are set using `hpx::lcos::async<>()`.
The `hello_world_foreman_action` is declared here:

[hello_world_action_wrapper]

Another way of thinking about this wrapping technique is as follows: functions
(the work to be done) are wrapped in actions, and actions can be executed
locally or remotely (e.g. on another machine participating in the computation).

Now it is time to look at the `hello_world_foreman()` function which was wrapped in
the action above: 

[hello_world_foreman]

Now, before we discuss `hello_world_foreman()`, let's talk about the
`hpx::lcos::wait()` function. `hpx::lcos::wait()` provides a way to make sure
that all of the futures have finished being calculated without having to call
`get()` for each one. The version of `hpx::lcos::wait()` used here performs an
non-blocking wait, which acts on an `std::vector<>`. It queries the state of
the futures, waiting for them to finish. Whenever a future becomes marked as
ready, `hpx::lcos::wait()` invokes a callback function provided by the user,
supplying the callback function with the index of the future in the
`std::vector<>` and the result of the future.

In `hello_world_foreman()`, an `std::set<>` called `attendance` keeps track of
which OS-threads have printed out the hello world message. When the OS-thread
prints out the statement, the future is marked as ready, and
`hpx::lcos::wait()` invokes the callback function, in this case a C+11 lambda.
This lambda erases the OS-threads id from the set `attendance`, thus letting
`hello_world_foreman()` know which OS-threads still need to print out hello
world. However, if the future returns a value of -1, the future executed on an
OS-thread which has already printed out hello world. In this case, we have to
try again by rescheduling the future in the next round. We do this by leaving
the OS-thread id in `attendance`.

Finally, let us look at `hello_world_worker()`. Here, `hello_world_worker()`
checks to see if it is on the target OS-thread. If it is executing on the
correct OS-thread, it prints out the hello world message and returns the
OS-thread id to `hpx::lcos::wait()` in `hello_world_foreman()`. If it is
not executing on the correct OS-thread, it returns a value of -1, which causes
`hello_world_foreman()` to leave the OS-thread id in `attendance`.

[hello_world_worker]

Because HPX features work stealing task schedulers, there is no way to guarantee
that an action will be scheduled on a particular OS-thread. This is why we must
use a guess-and-check approach. 

[endsect] [/Hello World]

[/Proofread by:]
[/Adrian Serio 3-13-12]
[/Phillip LeBlanc 3-13-12]

[/////////////////////////////////////////////////////////////////////////////]
[section:accumulator Accumulator]

The accumulator example demonstrates the use of components. Components are C++
classes that expose methods as a type of HPX action. These actions are called
component actions.

Components are globally named, meaning that a component action can be called
remotely (e.g.  from another machine). There are two accumulator examples in
HPX; managed_accumulator and simple_accumulator (we will talk more about the
differences between the two later). This tutorial will examine the
managed_accumulator variant.

In the __fibonacci_example__ and the
__hello_world_example__, we introduced plain actions, which wrapped global
functions.  The target of a plain action is an identifier which refers to a
particular machine involved in the computation. For plain actions, the target
is the machine where the action will be executed. 

Component actions, however, do not target machines. Instead, they target
component instances. The instance may live on the machine that we've invoked
the component action from, or it may live on another machine. 

The component in this example exposes three different functions:

* `reset()` - Resets the accumulator value to 0.
* `add(arg)` - Adds `arg` to the accumulators value.
* `query()` - Queries the value of the accumulator.

This example creates an instance of the accumulator, and then allows the user
to enter commands at a prompt, which subsequently invoke actions on the
accumulator instance.

[heading Setup]

[teletype]

The source code for this example can be found
[hpx_link examples/accumulator/accumulators..here]. 

To compile this program, go to your HPX build directory (see __getting_started__
for information on configuring and building HPX) and enter:

``
    $ make examples.accumulator.managed_accumulator
``

To run the program type:


``
    ./bin/managed_accumulator_client
``

Once the program starts running, it will print the following prompt and then
wait for input. An example session is given below:

``
    commands: reset, add [amount], query, help, quit
    > add 5
    > add 10
    > query
    15
    > add 2
    > query
    17
    > reset
    > add 1
    > query
    1
    > quit
``

[heading Walkthrough]

[c++]

Now, let's take a look at the source code of the managed_accumulator example. This
example consists of two parts: an HPX component library (a library that exposes
an HPX component) and a client application which uses the library. This
walkthrough will cover the HPX component library. The code for the client
application can be found [hpx_link examples/accumulator/managed_accumulator_client.cpp..here]. 

An HPX component is represented by three C++ classes:

* [*A server class] - The implementation of the components functionality.
* [*A stubs class] - A lower-level interface to instances of the component.
* [*A client class] - A high-level interface that acts as a proxy for an
  instance of the component. 

Typically, these three classes all have the same name, but stubs and server
classes usually live in different sub-namespaces (`server` and `stubs`
respectively).  For example, the full names of the three classes in
managed_accumulator are:

* `examples::server::managed_accumulator` (server class)
* `examples::stubs::managed_accumulator` (stubs class)
* `examples::managed_accumulator` (client class)

[heading The Server Class]

The following code is from
[hpx_link examples/accumulator/accumulators/server/managed_accumulator.hpp..server/managed_accumulator.hpp].

All HPX component server classes must inherit publicly from an HPX component
base class. There are currently two component base classes:

* `hpx::components::managed_component_base<>` - Managed components are components
  which are allocated in bulk by HPX. Managed components are more efficient if
  you are creating a large number (e.g. hundreds or more per machine) of
  component instances.
* `hpx::components::simple_component_base<>` - Simple components are components
  which are allocated individually by HPX. Simple components are more efficient
  if you are creating a small number (e.g. only a handful per machine) of
  component instances.

The managed_accumulator component is a managed component, because it inherits
from `hpx::components::managed_component_base`, as we can see in the following
snippet:

[import ../examples/accumulator/accumulators/server/managed_accumulator.hpp]
[managed_accumulator_server_inherit]

Our accumulator class will need a data member to store its value in, so let's
declare a data member:

[managed_accumulator_server_data_member]

The `boost::atomic<>` template class is from the __boost_atomic__ library. We use
this class to ensure thread-safety in our example. Think of this data member as
a thread-safe, 64-bit unsigned integer.

The constructor for this class simply initializes `value_` to 0:

[managed_accumulator_server_ctor]

Next, let's look at the three methods of this component that we will be exposing
as component actions:

[managed_accumulator_methods]

Before we can define the action types for these methods, we first need to
associate an action code (an arbitrary, unique integral value) with each method
that we are going to expose as an action:

[managed_accumulator_action_codes]

Here are the action types. These types wrap the methods we're exposing. The
wrapping technique is very similar to the one used in the __fibonacci_example__
and the __hello_world_example__:

[managed_accumulator_action_types]

The last piece of code in the server class header is the declaration of the
action type registration code:

[managed_accumulator_registration_declarations]

[note The code above must be placed in the global namespace.]

The rest of the registration code is in 
[hpx_link examples/accumulator/accumulators/managed_accumulator.cpp..managed_accumulator.cpp].

[import ../examples/accumulator/accumulators/managed_accumulator.cpp]
[managed_accumulator_registration_definitions]

[note The code above must be placed in the global namespace.]

[heading The Stubs Class]

The following code is from
[hpx_link examples/accumulator/accumulators/stubs/managed_accumulator.hpp..stubs/managed_accumulator.hpp].

All stubs classes must inherit from the stubs base class, `hpx::components::stub_base<>`:

[import ../examples/accumulator/accumulators/stubs/managed_accumulator.hpp]
[managed_accumulator_stubs_inherit]

The stubs class contains helper functions which invoke actions on component
instances. There are a few different ways of invoking actions:

* [*Non-blocking]: For actions which don't have return types, or when we do not
  care about the result of an action, we can invoke the action using
  fire-and-forget semantics. This means that once we have asked HPX to compute
  the action, we forget about it completely and continue with our computation.
  We use `hpx::applier::apply<>()` instead of `hpx::lcos::async<>()` to invoke
  an action in a non-blocking fashion. Here's an example from the
  managed_accumulator stubs class:

[managed_accumulator_stubs_reset_non_blocking]

* [*Asynchronous]: Futures, as demonstrated in __fibonacci_example__ and the
  __hello_world_example__, enable asynchronous action invocation. Here's an
  example from the managed_accumulator stubs class:

[managed_accumulator_stubs_query_async]

* [*Synchronous]: To invoke an action in a fully synchronous manner, we can
  simply call `hpx::lcos::async<>().get()` (e.g., create a future and immediately
  wait on it to be ready). Here's an example from the managed_accumulator stubs
  class:

[managed_accumulator_stubs_add_sync]

`hpx::naming::id_type` is a type which represents a global identifier in HPX.
This is the type that is returned by `hpx::find_here()`. This type specifies
the target of an action.

[heading The Client Class]

The following code is from
[hpx_link examples/accumulator/accumulators/managed_accumulator.hpp..managed_accumulator.hpp].

The client class is the primary interface to a component instance. Client classes
are used to create components:

``
    examples::managed_accumulator c;
    c.create(hpx::find_here()); // Create a component on this machine.
``

and to invoke component actions:

``
    c.add_sync(4);
``

Clients, like stubs and servers, need to inherit from a base class, this time,
`hpx::components::client_base<>`:

[import ../examples/accumulator/accumulators/managed_accumulator.hpp]
[managed_accumulator_client_inherit]

For readability, we typedef the base class like so:

[managed_accumulator_base_type]

Here are examples of how to expose actions through a client class:

[managed_accumulator_client_reset_non_blocking]

[managed_accumulator_client_query_async]

[managed_accumulator_client_add_sync]

Note that `this->gid_` references a data member of the
`hpx::components::client_base<>` base class.

[endsect] [/Accumulator]

[/Proofread by:]
[/Phillip LeBlanc 3-13-12]

[/////////////////////////////////////////////////////////////////////////////]

[endsect] [/Examples]

