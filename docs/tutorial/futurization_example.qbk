[/=============================================================================
   Copyright (c) 2014 Adrian Serio 
     
   Distributed under the Boost Software License, Version 1.0. (See accompanying
   file LICENSE_1_0.txt or copy at http://www.boost.org/LICENSE_1_0.txt)
=============================================================================/]

[section:futurization_example Futurization Example]

The following set of expamples were developed to demonstrate how a 
serial program 
can use the tecniques of futurization to exploit the parallism of an
algorithm.  In this case, we implemented a 1D heat distribution problem.
The first example is straight serial code.  In this code we instaniate
 a vector which contains two vectors of doubles.  
Each element in the vector of doubles represents a single grid
point. To calculate the change in heat distibution, the temperature of 
each grid point, along with its neighbors, are passed to the function heat.
 In order to imporove readablility, references named current 
and next are created which, depending on the timestep, point to the first
and second vector of doubles. The first vector of doubles is initialized 
with a simple 
heat ramp. After calling the heat function with the data in the "current"
vector, the results are placed into the "next" vector. 

In example 2 the code is futureized.  To do this we re-define our
our partion type as a shared_future [ADD TELETYPE] and create 
the object "result" which is a future vector of shared futures. We
then call result.get() and set its value equal to the solution vector.
The future vector "result" is calculated by stringing the futrue results 
of the function heat together so that each element calls heat 
when the previous futures it depends on are ready. 
In HPX 
we have an LCO whch assists the programmer in this process.  Dataflow 
allows us to pass the results a set of futures to a specified function 
when the futures are ready.
Dataflow talkes [x] arguments which intruct the object on how to perform 
the function call, the funcion to call, and the arguments for that function. 
When called, dataflow immediatly returns a future to the result of the 
specified function.  This allows users to string the dataflows together 
and construnct an execution tree. 

After the values of the futues in data flow are ready the values must 
be prepared to be passed to the function heat. In order to do this we 
use the HPX facility unwrapped.  In this example, unwrapped
retrieves the value of the futuers passed to Op and passes thier
values to the function heat.

By setting up the algorithm this way, the program will be able to exexute
as the dependecies are meet.  Unfortunaly, this example runs 
terribly slow. This increase in execution time is caused 
by the increased overheads needed to create a future for each
data point. Because the work done within each call to heat is very
small, the overhead of creating and schedualing each of the three
futures is greater than that of the actual useful work! In order
amatorize [SP] the oveerheads of our syncronization techniques,
we need to be able to control the amount of work that with be
done with each future.  We call this amout of work per overhead
grain size.

In example 3 we retun to our serial code to figure out how to 
control the grain size of our program. The stratagy that we 
employ is to create "partiions" of data points. The user
can define how many partitions are created and how many 
datapoints are contained in each partition. This is accomplished 
by creating the stuct partition and defineing space to now
be a vector of partions.

In example 4 we take advangate of the partion setup by redefining
space to be a vector of shared_futures whaich represent partitions.
In hpx_main we create a future vector of space named result which
contains the final calculated data points and set it equal
to the step member function do_work.  We pass the number 
of partions, the number of grid points per partition, and 
the number of steps that the program will simulate to this
function, which sets up the execution tree.  It should be noted
how strikingly similar example 4 is to example 2.

Example 4 finaly shows us some good results.  This code 
is faster than the OpenMP version by a factor of [INSERT FACTOR HERE].
While these results are impressive our work on this
algorithm is not over. This example only runs on one locality.
To get the full benefit of HPX we need to be albe to distibute
the work to other machines in a cluster. We begin this process in
example 5.

In order to run in distributed a large amount of boilerplate code
must be added. Fortunatley, HPX provides us with the concept of
a component which saves us from having to write quite as much code.
 A component simply is an object which can be 
remotely accessed using its global address. Components
are made of two parts: a server and a client class.
While it is not required to write them this way, abstracting 
the server and the client allow us to hide the boiler plate code 
and ensure type safety instead of having to pass around pointers
to global objects. Example 5 renames example 4's 
struct partiiton [TELETYPE] to partition_data [TELETYPE] and
adds serialization support. Next we add the server side
representation of the data in the structure partition_server [TELETYPE].
This object's public members are accessable anywhere on the
machine via its Global Identifyer (GID). To encapsulate
the component, we create a client side helper class.  This
object allows us to create new instances of our component,
and access its members without having to know its GID. In addition,
 we are using the client class to assist us with managing 
our asyncrony and futures.

In the structure stepper, we have also had to make some changes
to accomadate a distributed environment.  In order to get the data from a 
neighboring partition, which could be remote, we must retreve the 
data from the neiboring partitions. These retrevals should be asycronous
and the funciton heat_part_data [TELETYPE], which amongst other things calls
heat, should not be called unless the data from the neigboring partitions
have arrived.  Therefore it should come as no supprize that we syncronize 
this operarion with another instance of dataflow.
...talk about the unwrapped in heat_part...


[/////////////////////////////////////////////////////////////////////////////]

